TensorFlow 1.x selected.
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
WARNING:tensorflow:From mask_grasp_rcnn.py:1019: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From mask_grasp_rcnn.py:1021: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-10-19 20:51:22.036170: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2020-10-19 20:51:22.043736: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000160000 Hz
2020-10-19 20:51:22.044103: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1429480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-19 20:51:22.044136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-19 20:51:22.056555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-10-19 20:51:22.238409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:22.239180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1428bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-19 20:51:22.239230: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-10-19 20:51:22.239485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:22.240147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-10-19 20:51:22.246042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 20:51:22.264307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-19 20:51:22.272015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-19 20:51:22.277862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-19 20:51:22.288884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-19 20:51:22.292272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-19 20:51:22.315206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-19 20:51:22.315422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:22.316128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:22.316661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-10-19 20:51:22.316779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 20:51:22.318530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-19 20:51:22.318561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-10-19 20:51:22.318573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-10-19 20:51:22.318781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:22.319451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:22.322180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /content/drive/My Drive/object_vs_background/mrcnn/model.py:1250: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.
Instructions for updating:
box_ind is deprecated, use box_indices instead
2020-10-19 20:51:32.426008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.426724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-10-19 20:51:32.426828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 20:51:32.426848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-19 20:51:32.426866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-19 20:51:32.426914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-19 20:51:32.426942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-19 20:51:32.426972: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-19 20:51:32.427008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-19 20:51:32.427098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.427733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.428411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-10-19 20:51:32.429175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.429781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-10-19 20:51:32.429837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 20:51:32.429855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-19 20:51:32.429869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-19 20:51:32.429883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-19 20:51:32.429933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-19 20:51:32.429947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-19 20:51:32.429961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-19 20:51:32.430029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.430647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.431204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2020-10-19 20:51:32.431246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-19 20:51:32.431258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2020-10-19 20:51:32.431265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2020-10-19 20:51:32.431356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.431957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-19 20:51:32.432518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.
Instructions for updating:
reduction_indices is deprecated, use axis instead

Starting at epoch 0. LR=0.002

Checkpoint Path: /content/drive/My Drive/models/grasp_and_mask20201019T2051/mask_rcnn_grasp_and_mask_{epoch:04d}.h5
Selecting layers to train
In model:  rpn_model
grasp_res3a_branch2a   (TimeDistributed)
grasp_bn3a_branch2a    (TimeDistributed)
grasp_res3a_branch2b   (TimeDistributed)
grasp_bn3a_branch2b    (TimeDistributed)
grasp_res3a_branch2c   (TimeDistributed)
grasp_res3a_branch1    (TimeDistributed)
grasp_bn3a_branch2c    (TimeDistributed)
grasp_bn3a_branch1     (TimeDistributed)
grasp_res3b_branch2a   (TimeDistributed)
grasp_bn3b_branch2a    (TimeDistributed)
grasp_res3b_branch2b   (TimeDistributed)
grasp_bn3b_branch2b    (TimeDistributed)
grasp_res3b_branch2c   (TimeDistributed)
grasp_bn3b_branch2c    (TimeDistributed)
grasp_res3c_branch2a   (TimeDistributed)
grasp_bn3c_branch2a    (TimeDistributed)
grasp_res3c_branch2b   (TimeDistributed)
grasp_bn3c_branch2b    (TimeDistributed)
grasp_res3c_branch2c   (TimeDistributed)
grasp_bn3c_branch2c    (TimeDistributed)
grasp_res4a_branch2a   (TimeDistributed)
grasp_bn4a_branch2a    (TimeDistributed)
grasp_res4a_branch2b   (TimeDistributed)
grasp_bn4a_branch2b    (TimeDistributed)
grasp_res4a_branch2c   (TimeDistributed)
grasp_res4a_branch1    (TimeDistributed)
grasp_bn4a_branch2c    (TimeDistributed)
grasp_bn4a_branch1     (TimeDistributed)
grasp_res4b_branch2a   (TimeDistributed)
grasp_bn4b_branch2a    (TimeDistributed)
grasp_res4b_branch2b   (TimeDistributed)
grasp_bn4b_branch2b    (TimeDistributed)
grasp_res4b_branch2c   (TimeDistributed)
grasp_bn4b_branch2c    (TimeDistributed)
grasp_res4c_branch2a   (TimeDistributed)
grasp_bn4c_branch2a    (TimeDistributed)
grasp_res4c_branch2b   (TimeDistributed)
grasp_bn4c_branch2b    (TimeDistributed)
grasp_res4c_branch2c   (TimeDistributed)
grasp_bn4c_branch2c    (TimeDistributed)
grasp_class_conv       (TimeDistributed)
grasp_reg_conv         (TimeDistributed)
grasp_class_bn         (TimeDistributed)
grasp_reg_bn           (TimeDistributed)
grasp_class_raw        (TimeDistributed)
grasp_bbox_pred        (TimeDistributed)
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

/tensorflow-1.15.2/python3.6/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/500
2020-10-19 20:53:32.244671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-19 20:53:32.636570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-19 20:53:40.723484: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:40.723594: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:43.293479: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:43.293587: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:45.655554: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.79GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:45.655640: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.79GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:45.965919: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.84GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 20:53:45.966011: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.84GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
300/300 [==============================] - 731s 2s/step - loss: 2.1085 - rpn_class_loss: 0.0071 - rpn_bbox_loss: 0.3603 - mrcnn_class_loss: 0.1223 - mrcnn_bbox_loss: 0.3239 - mrcnn_mask_loss: 0.5005 - grasp_loss: 0.7945 - val_loss: 1.7614 - val_rpn_class_loss: 0.0051 - val_rpn_bbox_loss: 0.2986 - val_mrcnn_class_loss: 0.1496 - val_mrcnn_bbox_loss: 0.3229 - val_mrcnn_mask_loss: 0.4241 - val_grasp_loss: 0.6854
WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

Epoch 2/500
300/300 [==============================] - 696s 2s/step - loss: 1.9995 - rpn_class_loss: 0.0088 - rpn_bbox_loss: 0.3706 - mrcnn_class_loss: 0.1373 - mrcnn_bbox_loss: 0.3356 - mrcnn_mask_loss: 0.4711 - grasp_loss: 0.6759 - val_loss: 1.2249 - val_rpn_class_loss: 0.0057 - val_rpn_bbox_loss: 0.3126 - val_mrcnn_class_loss: 0.1247 - val_mrcnn_bbox_loss: 0.3195 - val_mrcnn_mask_loss: 0.4733 - val_grasp_loss: 0.6804
Epoch 3/500
300/300 [==============================] - 696s 2s/step - loss: 1.9233 - rpn_class_loss: 0.0060 - rpn_bbox_loss: 0.3507 - mrcnn_class_loss: 0.1118 - mrcnn_bbox_loss: 0.3048 - mrcnn_mask_loss: 0.4809 - grasp_loss: 0.6690 - val_loss: 2.0342 - val_rpn_class_loss: 0.0080 - val_rpn_bbox_loss: 0.3365 - val_mrcnn_class_loss: 0.1423 - val_mrcnn_bbox_loss: 0.3315 - val_mrcnn_mask_loss: 0.5610 - val_grasp_loss: 0.6660
Epoch 4/500
154/300 [==============>...............] - ETA: 5:09 - loss: 1.9427 - rpn_class_loss: 0.0078 - rpn_bbox_loss: 0.3557 - mrcnn_class_loss: 0.1390 - mrcnn_bbox_loss: 0.3370 - mrcnn_mask_loss: 0.4486 - grasp_loss: 0.65462020-10-19 21:34:10.900625: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-10-19 21:34:10.905807: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
300/300 [==============================] - 694s 2s/step - loss: 1.9386 - rpn_class_loss: 0.0072 - rpn_bbox_loss: 0.3442 - mrcnn_class_loss: 0.1362 - mrcnn_bbox_loss: 0.3338 - mrcnn_mask_loss: 0.4630 - grasp_loss: 0.6541 - val_loss: 2.0906 - val_rpn_class_loss: 0.0045 - val_rpn_bbox_loss: 0.2758 - val_mrcnn_class_loss: 0.1118 - val_mrcnn_bbox_loss: 0.3213 - val_mrcnn_mask_loss: 0.4023 - val_grasp_loss: 0.6423
Epoch 5/500
300/300 [==============================] - 695s 2s/step - loss: 1.8729 - rpn_class_loss: 0.0057 - rpn_bbox_loss: 0.3148 - mrcnn_class_loss: 0.1324 - mrcnn_bbox_loss: 0.3246 - mrcnn_mask_loss: 0.4447 - grasp_loss: 0.6506 - val_loss: 2.1537 - val_rpn_class_loss: 0.0075 - val_rpn_bbox_loss: 0.3455 - val_mrcnn_class_loss: 0.1165 - val_mrcnn_bbox_loss: 0.3271 - val_mrcnn_mask_loss: 0.4946 - val_grasp_loss: 0.6364
Epoch 6/500
300/300 [==============================] - 694s 2s/step - loss: 1.7861 - rpn_class_loss: 0.0058 - rpn_bbox_loss: 0.3293 - mrcnn_class_loss: 0.0996 - mrcnn_bbox_loss: 0.2778 - mrcnn_mask_loss: 0.4272 - grasp_loss: 0.6463 - val_loss: 1.6173 - val_rpn_class_loss: 0.0076 - val_rpn_bbox_loss: 0.4407 - val_mrcnn_class_loss: 0.1253 - val_mrcnn_bbox_loss: 0.3276 - val_mrcnn_mask_loss: 0.4714 - val_grasp_loss: 0.6382
Epoch 7/500
300/300 [==============================] - 694s 2s/step - loss: 1.8234 - rpn_class_loss: 0.0073 - rpn_bbox_loss: 0.3401 - mrcnn_class_loss: 0.1179 - mrcnn_bbox_loss: 0.2994 - mrcnn_mask_loss: 0.4150 - grasp_loss: 0.6436 - val_loss: 1.1256 - val_rpn_class_loss: 0.0067 - val_rpn_bbox_loss: 0.2538 - val_mrcnn_class_loss: 0.1057 - val_mrcnn_bbox_loss: 0.2696 - val_mrcnn_mask_loss: 0.3991 - val_grasp_loss: 0.6257
Epoch 8/500
300/300 [==============================] - 695s 2s/step - loss: 1.9457 - rpn_class_loss: 0.0077 - rpn_bbox_loss: 0.3765 - mrcnn_class_loss: 0.1279 - mrcnn_bbox_loss: 0.3415 - mrcnn_mask_loss: 0.4595 - grasp_loss: 0.6326 - val_loss: 1.0586 - val_rpn_class_loss: 0.0054 - val_rpn_bbox_loss: 0.3538 - val_mrcnn_class_loss: 0.0814 - val_mrcnn_bbox_loss: 0.2729 - val_mrcnn_mask_loss: 0.4844 - val_grasp_loss: 0.6135
Epoch 9/500
300/300 [==============================] - 695s 2s/step - loss: 1.7201 - rpn_class_loss: 0.0055 - rpn_bbox_loss: 0.3265 - mrcnn_class_loss: 0.1006 - mrcnn_bbox_loss: 0.2674 - mrcnn_mask_loss: 0.3915 - grasp_loss: 0.6286 - val_loss: 2.5337 - val_rpn_class_loss: 0.0054 - val_rpn_bbox_loss: 0.2745 - val_mrcnn_class_loss: 0.1100 - val_mrcnn_bbox_loss: 0.2649 - val_mrcnn_mask_loss: 0.4102 - val_grasp_loss: 0.6310
Epoch 10/500
300/300 [==============================] - 696s 2s/step - loss: 1.7327 - rpn_class_loss: 0.0051 - rpn_bbox_loss: 0.2904 - mrcnn_class_loss: 0.1006 - mrcnn_bbox_loss: 0.2878 - mrcnn_mask_loss: 0.4271 - grasp_loss: 0.6217 - val_loss: 2.3157 - val_rpn_class_loss: 0.0069 - val_rpn_bbox_loss: 0.3405 - val_mrcnn_class_loss: 0.1379 - val_mrcnn_bbox_loss: 0.3288 - val_mrcnn_mask_loss: 0.4828 - val_grasp_loss: 0.6372
Epoch 11/500
300/300 [==============================] - 696s 2s/step - loss: 2.0581 - rpn_class_loss: 0.0082 - rpn_bbox_loss: 0.4085 - mrcnn_class_loss: 0.1572 - mrcnn_bbox_loss: 0.3719 - mrcnn_mask_loss: 0.4841 - grasp_loss: 0.6283 - val_loss: 2.3274 - val_rpn_class_loss: 0.0102 - val_rpn_bbox_loss: 0.5095 - val_mrcnn_class_loss: 0.1275 - val_mrcnn_bbox_loss: 0.2905 - val_mrcnn_mask_loss: 0.4801 - val_grasp_loss: 0.6238
Epoch 12/500
300/300 [==============================] - 693s 2s/step - loss: 1.7068 - rpn_class_loss: 0.0047 - rpn_bbox_loss: 0.3061 - mrcnn_class_loss: 0.0975 - mrcnn_bbox_loss: 0.2624 - mrcnn_mask_loss: 0.4078 - grasp_loss: 0.6283 - val_loss: 1.8460 - val_rpn_class_loss: 0.0074 - val_rpn_bbox_loss: 0.3853 - val_mrcnn_class_loss: 0.1234 - val_mrcnn_bbox_loss: 0.3197 - val_mrcnn_mask_loss: 0.4282 - val_grasp_loss: 0.6235
Epoch 13/500
300/300 [==============================] - 693s 2s/step - loss: 1.8743 - rpn_class_loss: 0.0069 - rpn_bbox_loss: 0.3776 - mrcnn_class_loss: 0.1205 - mrcnn_bbox_loss: 0.3135 - mrcnn_mask_loss: 0.4280 - grasp_loss: 0.6278 - val_loss: 2.1482 - val_rpn_class_loss: 0.0071 - val_rpn_bbox_loss: 0.3443 - val_mrcnn_class_loss: 0.1459 - val_mrcnn_bbox_loss: 0.3418 - val_mrcnn_mask_loss: 0.5123 - val_grasp_loss: 0.6389
Epoch 14/500
300/300 [==============================] - 694s 2s/step - loss: 1.8895 - rpn_class_loss: 0.0070 - rpn_bbox_loss: 0.3562 - mrcnn_class_loss: 0.1246 - mrcnn_bbox_loss: 0.3432 - mrcnn_mask_loss: 0.4332 - grasp_loss: 0.6253 - val_loss: 2.5288 - val_rpn_class_loss: 0.0063 - val_rpn_bbox_loss: 0.2897 - val_mrcnn_class_loss: 0.1338 - val_mrcnn_bbox_loss: 0.3187 - val_mrcnn_mask_loss: 0.5143 - val_grasp_loss: 0.6162
Epoch 15/500
300/300 [==============================] - 693s 2s/step - loss: 1.8257 - rpn_class_loss: 0.0065 - rpn_bbox_loss: 0.3122 - mrcnn_class_loss: 0.1193 - mrcnn_bbox_loss: 0.3068 - mrcnn_mask_loss: 0.4586 - grasp_loss: 0.6224 - val_loss: 0.9572 - val_rpn_class_loss: 0.0072 - val_rpn_bbox_loss: 0.3864 - val_mrcnn_class_loss: 0.1294 - val_mrcnn_bbox_loss: 0.3544 - val_mrcnn_mask_loss: 0.4448 - val_grasp_loss: 0.6053
Epoch 16/500
 12/300 [>.............................] - ETA: 10:10 - loss: 1.9220 - rpn_class_loss: 0.0065 - rpn_bbox_loss: 0.2645 - mrcnn_class_loss: 0.1720 - mrcnn_bbox_loss: 0.4026 - mrcnn_mask_loss: 0.4669 - grasp_loss: 0.6096
